import asyncio
from playwright.async_api import async_playwright
import pandas as pd
from datetime import datetime
from flask import Flask, request, jsonify
import os

app = Flask(__name__)

# ========================================
# FUNCI√ìN DE SCRAPING (misma que ten√≠as)
# ========================================

# Usaremos un set para evitar duplicados si la paginaci√≥n se solapa
scraped_review_titles = set()
reviews_data = []

async def safe_get_text(card, selector, default=""):
    """Funci√≥n helper para extraer texto de forma segura."""
    try:
        locator = card.locator(selector).first
        if await locator.count() > 0:
            return (await locator.inner_text()).strip()
    except:
        pass
    return default

async def extract_reviews_from_page(page):
    """Extrae todas las rese√±as de la p√°gina actualmente visible."""
    global reviews_data, scraped_review_titles

    print("   üîç Buscando tarjetas de rese√±as en la p√°gina actual...")
    await page.wait_for_selector('div[data-testid="review-card"]', timeout=15000)
    review_cards = await page.locator('div[data-testid="review-card"]').all()
    print(f"   üìÑ Encontradas {len(review_cards)} rese√±as.")

    new_reviews_added = 0
    for card in review_cards:
        try:
            review_title = await safe_get_text(card, 'h4[data-testid="review-title"]')

            # Evitar duplicados
            if review_title and review_title in scraped_review_titles:
                continue

            scraped_review_titles.add(review_title)

            # --- Autor y Estancia ---
            user_name = await safe_get_text(card, 'div.b08850ce41')
            user_country = await safe_get_text(card, 'span.d838fb5f41')
            traveler_type = await safe_get_text(card, 'span[data-testid="review-traveler-type"]')
            review_date = await safe_get_text(card, 'span[data-testid="review-date"]')

            # --- Rating ---
            rating_raw = await safe_get_text(card, 'div[data-testid="review-score"] div.bc946a29db')
            rating = rating_raw.replace("Puntuaci√≥n: ", "").strip()

            # --- Comentarios Positivos (+) ---
            liked_text_full = await safe_get_text(card, 'div[data-testid="review-positive-text"]')
            liked_text = liked_text_full.splitlines()[-1].strip() if liked_text_full.splitlines() else ""

            # --- Comentarios Negativos (-) ---
            disliked_text_full = await safe_get_text(card, 'div[data-testid="review-negative-text"]')
            disliked_text = disliked_text_full.splitlines()[-1].strip() if disliked_text_full.splitlines() else ""

            reviews_data.append({
                "Rating": rating,
                "Review Title": review_title,
                "Liked Text": liked_text,
                "Disliked Text": disliked_text,
                "Review Date": review_date,
                "Traveler Type": traveler_type,
                "User Name": user_name,
                "User Country": user_country,
                "Scraped Timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
            new_reviews_added += 1

        except Exception as e:
            print(f"   ‚ùå Error en una rese√±a: {str(e)[:100]}...")

    print(f"   ‚ú® {new_reviews_added} rese√±as nuevas a√±adidas. Total: {len(reviews_data)}")


async def scrape_booking_reviews(url, filter_option="default", max_pages=2):
    """
    Scraper de Booking.com con filtros y paginaci√≥n.
    """
    global reviews_data, scraped_review_titles
    reviews_data = []
    scraped_review_titles = set()

    async with async_playwright() as p:
        print("üåê Iniciando navegador...")
        browser = await p.chromium.launch(
            headless=True,
            args=['--disable-gpu', '--disable-dev-shm-usage', '--no-sandbox']
        )
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36',
            locale='es-ES' # Pedir la p√°gina en espa√±ol
        )
        page = await context.new_page()

        try:
            print(f"‚û°Ô∏è Navegando a: {url}")
            await page.goto(url, timeout=90000, wait_until='networkidle')

            # --- 1. Manejar Cookies ---
            try:
                cookie_btn = page.locator('#onetrust-accept-btn-handler')
                if await cookie_btn.is_visible(timeout=5000):
                    print("üç™ Aceptando cookies...")
                    await cookie_btn.click()
                    await page.wait_for_timeout(1000)
            except:
                print("‚ÑπÔ∏è No se encontr√≥ el banner de cookies.")

            # --- 2. Ir a la secci√≥n de Rese√±as ---
            try:
                await page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
                read_all_btn = page.locator('button[data-testid="fr-read-all-reviews"]')
                if await read_all_btn.is_visible(timeout=5000):
                    print("‚úÖ Clic en 'Leer todos los comentarios'...")
                    await read_all_btn.click()
                    await page.wait_for_load_state('networkidle', timeout=15000)
                else:
                    print("‚ÑπÔ∏è Asumiendo que ya estamos en la p√°gina de rese√±as.")
            except Exception as e:
                print(f"‚ö†Ô∏è No se pudo hacer clic en 'Leer todos' (puede que no exista): {e}")

            # --- 3. Aplicar Filtro de Ordenamiento ---
            if filter_option != "default":
                try:
                    print(f"üîß Aplicando filtro: {filter_option}")
                    await page.locator('button[data-testid="sorters-dropdown-trigger"]').click()
                    await page.wait_for_timeout(500)
                    await page.locator(f'button[data-testid="sorters-dropdown-option-{filter_option}"]').click()
                    print("‚è≥ Esperando que el filtro se aplique...")
                    await page.wait_for_load_state('networkidle', timeout=15000)
                    print("‚úÖ Filtro aplicado.")
                except Exception as e:
                    print(f"‚ö†Ô∏è No se pudo aplicar el filtro: {e}")

            # --- 4. Loop de Paginaci√≥n ---
            for page_num in range(1, max_pages + 1):
                print("\n" + "="*30)
                print(f"üìÑ Extrayendo p√°gina {page_num} de {max_pages}...")

                await extract_reviews_from_page(page)

                if page_num < max_pages:
                    # Ir a la p√°gina siguiente
                    try:
                        next_btn = page.locator('button[aria-label="P√°gina siguiente"]')
                        if await next_btn.count() == 0 or not await next_btn.is_enabled():
                            print("üèÅ No hay m√°s p√°ginas. Terminando.")
                            break

                        print("‚è© Clic en 'P√°gina siguiente'...")
                        await next_btn.click()
                        await page.wait_for_load_state('networkidle', timeout=15000)
                    except Exception as e:
                        print(f"‚ùå Error al paginar: {e}")
                        break

        except Exception as e:
            print(f"‚ùå Error fatal durante el scraping: {e}")

        await browser.close()
        print(f"\n‚úÖ Scraping completado. Total de rese√±as √∫nicas: {len(reviews_data)}")
        return pd.DataFrame(reviews_data)

# ========================================
# FLASK API
# ========================================

@app.route('/scrape', methods=['POST'])
def handle_scrape():
    data = request.json
    hotel_name = data.get('hotel_name')

    if not hotel_name:
        return jsonify({"error": "No se proporcion√≥ nombre del hotel"}), 400

    # Aqu√≠ puedes construir la URL de Booking para el hotel
    # Por ejemplo, si tienes una forma de buscar el hotel en Booking
    # En este ejemplo, usaremos una URL fija como placeholder
    # Deber√°s implementar la l√≥gica de b√∫squeda real aqu√≠
    url = f"https://www.booking.com/hotel/pe/{hotel_name.replace(' ', '-').lower()}.es.html"

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        df = loop.run_until_complete(scrape_booking_reviews(url, max_pages=2))
        results = df.to_dict(orient='records')
        return jsonify(results)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/', methods=['GET'])
def home():
    return jsonify({"message": "API de scraping de hoteles en l√≠nea. Usa POST /scrape con {hotel_name}"})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)